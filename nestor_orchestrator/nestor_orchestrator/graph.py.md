# graph.py

## Описание

Модуль определяет граф обработки задач на базе LangGraph. Содержит определение состояния задачи, узлы обработки и скомпилированный граф выполнения.

## Changelog

### 2024-12-XX
- Создан модуль с базовым графом LangGraph
- Добавлен тип `TaskState` с полями `task_spec`, `plan`, `status`
- Реализован узел `intake_node` для создания плана выполнения задачи
- Добавлена интеграция с LLM (ChatOpenAI) для генерации планов
- Добавлена обработка ошибок LLM с дефолтным планом
- Добавлено поле `clarifying_questions` в `TaskState`
- Реализован узел `clarify_node` для генерации уточняющих вопросов через LLM
- Обновлена структура графа: `START -> intake -> clarify -> END`
- Добавлена поддержка переменных окружения через `python-dotenv`

## Структура

### Классы

#### `TaskState`
TypedDict, описывающий состояние задачи в графе.

**Поля:**
- `task_spec: str` - спецификация задачи
- `plan: list[str] | None` - план выполнения задачи (генерируется через LLM)
- `clarifying_questions: list[str] | None` - список уточняющих вопросов
- `status: Literal["new", "planned", "done"]` - статус обработки задачи

### Функции

#### `_get_llm_client() -> ChatOpenAI | None`
Создаёт и возвращает LLM-клиент для работы с OpenAI API.

**Возвращает:**
- `ChatOpenAI | None` - клиент LLM или `None`, если API ключ недоступен

**Использует:**
- Переменную окружения `OPENAI_API_KEY`
- Модель: `gpt-5.1`
- Temperature: `0.3`

#### `_parse_plan_from_llm_response(response_text: str) -> list[str]`
Парсит текстовый ответ LLM в список шагов плана.

**Параметры:**
- `response_text: str` - текст ответа от LLM

**Возвращает:**
- `list[str]` - список шагов плана

**Логика:**
- Разбивает текст по строкам
- Удаляет маркеры списка (1., -, *, • и т.д.)
- Удаляет нумерацию вида "Шаг 1:"
- Возвращает дефолтный план при пустом результате

#### `_parse_questions_from_llm_response(response_text: str) -> list[str]`
Парсит текстовый ответ LLM в список вопросов.

**Параметры:**
- `response_text: str` - текст ответа от LLM

**Возвращает:**
- `list[str]` - список уточняющих вопросов

**Логика:**
- Разбивает текст по строкам
- Удаляет маркеры списка и нумерацию
- Возвращает дефолтные вопросы при пустом результате

#### `intake_node(state: TaskState) -> TaskState`
Узел приёма задачи. Генерирует план выполнения через LLM.

**Параметры:**
- `state: TaskState` - текущее состояние задачи

**Возвращает:**
- `TaskState` - обновлённое состояние с планом и статусом "planned"

**Логика:**
- Если план отсутствует или пуст, вызывает LLM для генерации плана
- При ошибке LLM использует дефолтный план из двух шагов
- Устанавливает `status = "planned"`

#### `clarify_node(state: TaskState) -> TaskState`
Узел уточнения. Генерирует уточняющие вопросы через LLM.

**Параметры:**
- `state: TaskState` - текущее состояние задачи

**Возвращает:**
- `TaskState` - обновлённое состояние с уточняющими вопросами

**Логика:**
- Если вопросы отсутствуют или пусты, вызывает LLM для генерации 3-7 вопросов
- Использует `task_spec` и `plan` (если есть) для контекста
- При ошибке LLM использует дефолтные вопросы
- Не изменяет статус задачи

### Переменные

#### `_llm_client: ChatOpenAI | None`
Глобальный кэш LLM-клиента. Инициализируется при первом вызове `_get_llm_client()`.

#### `graph`
Скомпилированный граф LangGraph. Структура: `START -> intake -> clarify -> END`.

## Зависимости

- `langgraph` - для построения графа состояний
- `langchain-openai` - для работы с OpenAI API
- `python-dotenv` - для загрузки переменных окружения

